{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d59a7e",
   "metadata": {},
   "source": [
    "# Big Data — PySpark DataFrames + Kafka (confluent-kafka)\n",
    "This notebook demonstrates:\n",
    "- Basic **PySpark DataFrame** creation and transformations\n",
    "- A simple **Kafka Producer** and **Kafka Consumer** using the `confluent-kafka` Python library\n",
    "- Converting consumed Kafka messages into a Spark DataFrame\n",
    "\n",
    "Assumptions:\n",
    "- You are running inside the course Docker Compose network.\n",
    "- Kafka broker is reachable at `kafka:9092` (default), or via the env var `KAFKA_BOOTSTRAP_SERVERS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46864e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAFKA_BOOTSTRAP_SERVERS = kafka:9092\n",
      "TOPIC = sid45_demo\n",
      "GROUP_ID = sid45_demo_group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config ---\n",
    "import os, json, time, uuid\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka:9092\")\n",
    "TOPIC = os.getenv(\"KAFKA_TOPIC\", \"sid45_demo\")\n",
    "GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\", \"sid45_demo_group\")\n",
    "\n",
    "print(\"KAFKA_BOOTSTRAP_SERVERS =\", KAFKA_BOOTSTRAP)\n",
    "print(\"TOPIC =\", TOPIC)\n",
    "print(\"GROUP_ID =\", GROUP_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2570c0b",
   "metadata": {},
   "source": [
    "## 1) PySpark: create and transform DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b8a4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8d8c7a7762a1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sid45-pyspark-kafka-demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffa1126b90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"sid45-pyspark-kafka-demo\")\n",
    "         # For cluster mode (Spark standalone in Compose), uncomment:\n",
    "         # .master(\"spark://spark-master:7077\")\n",
    "         # For local mode (often simpler for a quick demo), keep as default:\n",
    "         .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb50c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|user_id|event   |value|\n",
      "+-------+--------+-----+\n",
      "|u1     |click   |1.2  |\n",
      "|u2     |view    |0.4  |\n",
      "|u1     |purchase|5.0  |\n",
      "|u3     |view    |0.2  |\n",
      "|u2     |click   |0.9  |\n",
      "+-------+--------+-----+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- value: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a small DataFrame\n",
    "data = [\n",
    "    (\"u1\", \"click\",  1.2),\n",
    "    (\"u2\", \"view\",   0.4),\n",
    "    (\"u1\", \"purchase\", 5.0),\n",
    "    (\"u3\", \"view\",   0.2),\n",
    "    (\"u2\", \"click\",  0.9),\n",
    "]\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), False),\n",
    "    T.StructField(\"event\", T.StringType(), False),\n",
    "    T.StructField(\"value\", T.DoubleType(), False),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfffa1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-----------+\n",
      "|user_id|event   |value|event_upper|\n",
      "+-------+--------+-----+-----------+\n",
      "|u1     |click   |1.2  |CLICK      |\n",
      "|u1     |purchase|5.0  |PURCHASE   |\n",
      "|u2     |click   |0.9  |CLICK      |\n",
      "+-------+--------+-----+-----------+\n",
      "\n",
      "+-------+--------+---------+-----------------+\n",
      "|user_id|n_events|sum_value|event_types      |\n",
      "+-------+--------+---------+-----------------+\n",
      "|u1     |2       |6.2      |[CLICK, PURCHASE]|\n",
      "|u2     |1       |0.9      |[CLICK]          |\n",
      "+-------+--------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple transformations: filter, groupBy, aggregation, and a derived column\n",
    "df2 = (df\n",
    "       .withColumn(\"event_upper\", F.upper(F.col(\"event\")))\n",
    "       .filter(F.col(\"value\") >= 0.5))\n",
    "\n",
    "agg = (df2\n",
    "       .groupBy(\"user_id\")\n",
    "       .agg(\n",
    "           F.count(\"*\").alias(\"n_events\"),\n",
    "           F.sum(\"value\").alias(\"sum_value\"),\n",
    "           F.collect_set(\"event_upper\").alias(\"event_types\")\n",
    "       )\n",
    "       .orderBy(F.desc(\"sum_value\")))\n",
    "\n",
    "df2.show(truncate=False)\n",
    "agg.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481a4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+-----------------+\n",
      "|user_id|n_events|sum_value|event_types      |\n",
      "+-------+--------+---------+-----------------+\n",
      "|u1     |2       |6.2      |[CLICK, PURCHASE]|\n",
      "|u2     |1       |0.9      |[CLICK]          |\n",
      "+-------+--------+---------+-----------------+\n",
      "\n",
      "Wrote and reloaded Parquet at: /workspace/data/output/pyspark_demo_parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: write/read Parquet to demonstrate I/O\n",
    "out_path = \"/workspace/data/output/pyspark_demo_parquet\"\n",
    "(agg.coalesce(1).write.mode(\"overwrite\").parquet(out_path))\n",
    "\n",
    "df_read = spark.read.parquet(out_path)\n",
    "df_read.show(truncate=False)\n",
    "\n",
    "print(\"Wrote and reloaded Parquet at:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a743ea",
   "metadata": {},
   "source": [
    "## 2) Kafka: create topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a487f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing topics (sample): [] ...\n",
      "Creating topic 'sid45_demo' (best-effort)...\n",
      "Created topic: sid45_demo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This cell tries to create the topic if it doesn't exist.\n",
    "# If Kafka is configured with auto-topic-creation, this may be unnecessary.\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin = AdminClient({\"bootstrap.servers\": KAFKA_BOOTSTRAP})\n",
    "\n",
    "# Check existing topics\n",
    "md = admin.list_topics(timeout=10)\n",
    "existing = set(md.topics.keys())\n",
    "print(\"Existing topics (sample):\", sorted(list(existing))[:10], \"...\")\n",
    "\n",
    "if TOPIC in existing:\n",
    "    print(f\"Topic '{TOPIC}' already exists.\")\n",
    "else:\n",
    "    print(f\"Creating topic '{TOPIC}' (best-effort)...\")\n",
    "    new_topic = NewTopic(topic=TOPIC, num_partitions=1, replication_factor=1)\n",
    "    fs = admin.create_topics([new_topic], request_timeout=15)\n",
    "    for t, f in fs.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f\"Created topic: {t}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Topic creation result for {t}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc414fdd",
   "metadata": {},
   "source": [
    "## 3) Kafka Producer\n",
    "We will produce a handful of JSON messages into the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439a6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivered to sid45_demo [0] @ offset 0\n",
      "Delivered to sid45_demo [0] @ offset 1\n",
      "Delivered to sid45_demo [0] @ offset 2\n",
      "Delivered to sid45_demo [0] @ offset 3\n",
      "Delivered to sid45_demo [0] @ offset 4\n",
      "Delivered messages: 5\n",
      "Batch ID: 8fe2db1f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "producer = Producer({\n",
    "    \"bootstrap.servers\": KAFKA_BOOTSTRAP,\n",
    "    \"retries\": 3,\n",
    "    \"linger.ms\": 50,\n",
    "})\n",
    "\n",
    "delivered = {\"count\": 0}\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(\"Delivery failed:\", err)\n",
    "    else:\n",
    "        delivered[\"count\"] += 1\n",
    "        print(f\"Delivered to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "batch_id = str(uuid.uuid4())[:8]\n",
    "messages = [\n",
    "    {\"batch\": batch_id, \"event_id\": 1, \"user_id\": \"u1\", \"event\": \"view\", \"value\": 0.3, \"ts\": time.time()},\n",
    "    {\"batch\": batch_id, \"event_id\": 2, \"user_id\": \"u2\", \"event\": \"click\", \"value\": 1.0, \"ts\": time.time()},\n",
    "    {\"batch\": batch_id, \"event_id\": 3, \"user_id\": \"u1\", \"event\": \"purchase\", \"value\": 5.5, \"ts\": time.time()},\n",
    "    {\"batch\": batch_id, \"event_id\": 4, \"user_id\": \"u3\", \"event\": \"view\", \"value\": 0.1, \"ts\": time.time()},\n",
    "    {\"batch\": batch_id, \"event_id\": 5, \"user_id\": \"u2\", \"event\": \"click\", \"value\": 0.7, \"ts\": time.time()},\n",
    "]\n",
    "\n",
    "for m in messages:\n",
    "    key = f\"{m['batch']}-{m['event_id']}\".encode(\"utf-8\")\n",
    "    value = json.dumps(m).encode(\"utf-8\")\n",
    "    producer.produce(TOPIC, key=key, value=value, on_delivery=delivery_report)\n",
    "    producer.poll(0)\n",
    "\n",
    "producer.flush(10)\n",
    "print(\"Delivered messages:\", delivered[\"count\"])\n",
    "print(\"Batch ID:\", batch_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0025dc",
   "metadata": {},
   "source": [
    "## 4) Kafka Consumer \n",
    "We will consume messages for a short time window and decode JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdfe06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumed 5 messages (within ~8s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'topic': 'sid45_demo',\n",
       "  'partition': 0,\n",
       "  'offset': 0,\n",
       "  'key': '8fe2db1f-1',\n",
       "  'value': {'batch': '8fe2db1f',\n",
       "   'event_id': 1,\n",
       "   'user_id': 'u1',\n",
       "   'event': 'view',\n",
       "   'value': 0.3,\n",
       "   'ts': 1771719623.057138}},\n",
       " {'topic': 'sid45_demo',\n",
       "  'partition': 0,\n",
       "  'offset': 1,\n",
       "  'key': '8fe2db1f-2',\n",
       "  'value': {'batch': '8fe2db1f',\n",
       "   'event_id': 2,\n",
       "   'user_id': 'u2',\n",
       "   'event': 'click',\n",
       "   'value': 1.0,\n",
       "   'ts': 1771719623.0571396}},\n",
       " {'topic': 'sid45_demo',\n",
       "  'partition': 0,\n",
       "  'offset': 2,\n",
       "  'key': '8fe2db1f-3',\n",
       "  'value': {'batch': '8fe2db1f',\n",
       "   'event_id': 3,\n",
       "   'user_id': 'u1',\n",
       "   'event': 'purchase',\n",
       "   'value': 5.5,\n",
       "   'ts': 1771719623.05714}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "\n",
    "consumer = Consumer({\n",
    "    \"bootstrap.servers\": KAFKA_BOOTSTRAP,\n",
    "    \"group.id\": GROUP_ID,\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "    \"enable.auto.commit\": False,\n",
    "})\n",
    "\n",
    "consumer.subscribe([TOPIC])\n",
    "\n",
    "received = []\n",
    "start = time.time()\n",
    "timeout_s = 8  # keep short to avoid hanging\n",
    "\n",
    "try:\n",
    "    while time.time() - start < timeout_s and len(received) < 20:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "\n",
    "        key = msg.key().decode(\"utf-8\") if msg.key() else None\n",
    "        val = msg.value().decode(\"utf-8\") if msg.value() else None\n",
    "        try:\n",
    "            obj = json.loads(val) if val else None\n",
    "        except json.JSONDecodeError:\n",
    "            obj = {\"raw\": val}\n",
    "\n",
    "        received.append({\n",
    "            \"topic\": msg.topic(),\n",
    "            \"partition\": msg.partition(),\n",
    "            \"offset\": msg.offset(),\n",
    "            \"key\": key,\n",
    "            \"value\": obj\n",
    "        })\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(f\"Consumed {len(received)} messages (within ~{timeout_s}s).\")\n",
    "received[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127b37b",
   "metadata": {},
   "source": [
    "## 5) Convert consumed messages into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4d6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+----------+------+---------+----------+--------------------+-------+-----+\n",
      "|batch   |event   |event_id|key       |offset|partition|topic     |ts                  |user_id|value|\n",
      "+--------+--------+--------+----------+------+---------+----------+--------------------+-------+-----+\n",
      "|8fe2db1f|view    |1       |8fe2db1f-1|0     |0        |sid45_demo|1.771719623057138E9 |u1     |0.3  |\n",
      "|8fe2db1f|click   |2       |8fe2db1f-2|1     |0        |sid45_demo|1.7717196230571396E9|u2     |1.0  |\n",
      "|8fe2db1f|purchase|3       |8fe2db1f-3|2     |0        |sid45_demo|1.77171962305714E9  |u1     |5.5  |\n",
      "|8fe2db1f|view    |4       |8fe2db1f-4|3     |0        |sid45_demo|1.7717196230571408E9|u3     |0.1  |\n",
      "|8fe2db1f|click   |5       |8fe2db1f-5|4     |0        |sid45_demo|1.771719623057141E9 |u2     |0.7  |\n",
      "+--------+--------+--------+----------+------+---------+----------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "rows = []\n",
    "for r in received:\n",
    "    v = r.get(\"value\") or {}\n",
    "    if isinstance(v, dict):\n",
    "        rows.append({\n",
    "            \"topic\": r[\"topic\"],\n",
    "            \"partition\": r[\"partition\"],\n",
    "            \"offset\": r[\"offset\"],\n",
    "            \"key\": r[\"key\"],\n",
    "            \"batch\": v.get(\"batch\"),\n",
    "            \"event_id\": v.get(\"event_id\"),\n",
    "            \"user_id\": v.get(\"user_id\"),\n",
    "            \"event\": v.get(\"event\"),\n",
    "            \"value\": v.get(\"value\"),\n",
    "            \"ts\": v.get(\"ts\"),\n",
    "        })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"topic\": r[\"topic\"],\n",
    "            \"partition\": r[\"partition\"],\n",
    "            \"offset\": r[\"offset\"],\n",
    "            \"key\": r[\"key\"],\n",
    "            \"batch\": None,\n",
    "            \"event_id\": None,\n",
    "            \"user_id\": None,\n",
    "            \"event\": None,\n",
    "            \"value\": None,\n",
    "            \"ts\": None,\n",
    "        })\n",
    "\n",
    "df_kafka = spark.createDataFrame(rows)\n",
    "df_kafka.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc18b514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|user_id|n_msgs|sum_value|\n",
      "+-------+------+---------+\n",
      "|u1     |2     |5.8      |\n",
      "|u2     |2     |1.7      |\n",
      "|u3     |1     |0.1      |\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A small Spark analysis on consumed events\n",
    "if df_kafka.count() > 0:\n",
    "    df_kafka.groupBy(\"user_id\").agg(\n",
    "        F.count(\"*\").alias(\"n_msgs\"),\n",
    "        F.sum(\"value\").alias(\"sum_value\")\n",
    "    ).orderBy(F.desc(\"sum_value\")).show(truncate=False)\n",
    "else:\n",
    "    print(\"No messages consumed — check Kafka service / topic / networking.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
